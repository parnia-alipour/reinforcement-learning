{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-01-21T12:19:07.751052Z",
     "start_time": "2026-01-21T12:19:07.746401Z"
    }
   },
   "source": [
    "import gymnasium\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Dense,Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from collections import deque\n",
    "import random\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 151
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T12:19:07.947026Z",
     "start_time": "2026-01-21T12:19:07.943750Z"
    }
   },
   "cell_type": "code",
   "source": "env=gymnasium.make('CartPole-v1',render_mode=\"human\")",
   "id": "44a0118542444b1b",
   "outputs": [],
   "execution_count": 152
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T12:19:08.167852Z",
     "start_time": "2026-01-21T12:19:08.141366Z"
    }
   },
   "cell_type": "code",
   "source": "env.reset()",
   "id": "cfbed9af0e72e52",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.02996851, -0.02417919, -0.03962907,  0.02692981], dtype=float32),\n",
       " {})"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 153
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T12:19:08.329879Z",
     "start_time": "2026-01-21T12:19:08.325653Z"
    }
   },
   "cell_type": "code",
   "source": "env.render()",
   "id": "3dfee3f9cf130bfa",
   "outputs": [],
   "execution_count": 154
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T12:19:08.525357Z",
     "start_time": "2026-01-21T12:19:08.522600Z"
    }
   },
   "cell_type": "code",
   "source": "print(env.observation_space.shape)",
   "id": "d551a9464b405037",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4,)\n"
     ]
    }
   ],
   "execution_count": 155
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T12:19:08.732450Z",
     "start_time": "2026-01-21T12:19:08.729430Z"
    }
   },
   "cell_type": "code",
   "source": "print(env.action_space.n)",
   "id": "8d06b3fdebfb5fa9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "execution_count": 156
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T12:19:08.971538Z",
     "start_time": "2026-01-21T12:19:08.967335Z"
    }
   },
   "cell_type": "code",
   "source": "env.action_space.sample()",
   "id": "5ff5a66f83f5e3ba",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(1)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 157
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T12:19:09.298534Z",
     "start_time": "2026-01-21T12:19:09.291064Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class DQN:\n",
    "    def __init__(self,env):\n",
    "        self.env = env\n",
    "        self.memory=deque(maxlen=2000)\n",
    "        self.gamma=0.99\n",
    "        self.epsilon=1.00\n",
    "        self.epsilon_min=0.01\n",
    "        self.epsilon_decay=0.995\n",
    "        self.learning_rate=0.001\n",
    "        self.tau=0.125\n",
    "        self.model=self.create_model()\n",
    "        self.target_model=self.create_model()\n",
    "    def create_model(self):\n",
    "            model = Sequential()\n",
    "            input_dim = env.observation_space.shape[0]\n",
    "            model.add(Dense(24, input_dim=input_dim,activation='relu'))\n",
    "            model.add(Dense(34, activation='relu'))\n",
    "            model.add(Dense(44, activation='relu'))\n",
    "            model.add(Dense(self.env.action_space.n ,activation='linear'))\n",
    "            model.compile(loss='mean_squared_error', optimizer=Adam(learning_rate=self.learning_rate))\n",
    "            return model\n",
    "    def act(self,state):\n",
    "            if np.random.rand()<self.epsilon:\n",
    "                return env.action_space.sample()\n",
    "            else:\n",
    "                state=np.array(state,dtype=np.float32).reshape(1,-1)\n",
    "                return np.argmax(self.model.predict(state))\n",
    "    def replay(self,batch_size=50):\n",
    "        if len(self.memory)<batch_size:\n",
    "            return\n",
    "\n",
    "        samples = random.sample(self.memory, batch_size)\n",
    "        states = np.vstack([s[0] for s in samples])\n",
    "        targets = self.model.predict(states, verbose=0)\n",
    "        for i, sample in enumerate(samples):\n",
    "            state, action, reward, new_state, done = sample\n",
    "            if done:\n",
    "                targets[i][action] = reward\n",
    "            else:\n",
    "                Q_future=max(self.target_model.predict(new_state,verbose=0)[0])\n",
    "                targets[i][action]=reward+self.gamma*Q_future\n",
    "\n",
    "        self.model.fit(states,targets,epochs=1,verbose=0)\n",
    "\n",
    "    def remember(self,state,action,reward,new_state,done):\n",
    "        self.memory.append([state,action,reward,new_state,done])\n",
    "    def target_train(self):\n",
    "        weights=self.model.get_weights()\n",
    "        target_weights=self.target_model.get_weights()\n",
    "        for i in range(len(target_weights)):\n",
    "            target_weights[i]=weights[i]*self.tau+target_weights[i]*(1-self.tau)\n",
    "        self.target_model.set_weights(target_weights)\n",
    "\n",
    "    def save_model(self,fn):\n",
    "        self.model.save(fn)\n",
    "\n"
   ],
   "id": "d8d95ac71157302a",
   "outputs": [],
   "execution_count": 158
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T12:19:09.626278Z",
     "start_time": "2026-01-21T12:19:09.581491Z"
    }
   },
   "cell_type": "code",
   "source": [
    "gamma=0.99\n",
    "epsilon=0\n",
    "dqn_model=DQN(env=env)"
   ],
   "id": "dd06d59c861e338",
   "outputs": [],
   "execution_count": 159
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T12:19:11.548080Z",
     "start_time": "2026-01-21T12:19:10.205095Z"
    }
   },
   "cell_type": "code",
   "source": [
    "trails=1000\n",
    "max_steps=100\n",
    "batch_size=50\n",
    "for episodes in range(trails):\n",
    "    state,_=env.reset()\n",
    "    c_state=state.reshape(1,-1)\n",
    "    steps_counts=0\n",
    "    total_reward=0\n",
    "    for step in range(max_steps):\n",
    "        steps_counts+=1\n",
    "        action=dqn_model.act(c_state)\n",
    "        new_state,reward,terminated,truncated,_=env.step(action)\n",
    "        done=terminated or truncated\n",
    "        dqn_model.remember(c_state,action,reward,new_state,done)\n",
    "        c_state=new_state.reshape(1,-1).astype(np.float32)\n",
    "        dqn_model.replay(batch_size=batch_size)\n",
    "        dqn_model.target_train()\n",
    "        total_reward+=reward\n",
    "        print(f\"\\r Episode {episodes+1} steps {steps_counts}\", end=\"\")\n",
    "        if done :\n",
    "            break\n",
    "    dqn_model.epsilon=max(dqn_model.epsilon_min,dqn_model.epsilon*dqn_model.epsilon_decay)\n",
    "    print(f\" Episode: {episodes+1}/{trails} steps: {steps_counts} total reward: {total_reward:.2f} Epsilon: {dqn_model.epsilon:.2f}\")\n",
    "    dqn_model.save_model(\"snow.keras\")\n",
    "\n"
   ],
   "id": "ad45b3e6dfcff3bc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Episode 1 steps 11 Episode: 1/1000 steps: 11 total reward: 11.00 Epsilon: 0.99\n",
      " Episode 2 steps 13 Episode: 2/1000 steps: 13 total reward: 13.00 Epsilon: 0.99\n",
      " Episode 3 steps 18 Episode: 3/1000 steps: 18 total reward: 18.00 Epsilon: 0.99\n",
      " Episode 4 steps 7"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling Sequential.call().\n\n\u001B[1mInvalid input shape for input Tensor(\"data:0\", shape=(4,), dtype=float32). Expected shape (None, 4), but input has incompatible shape (4,)\u001B[0m\n\nArguments received by Sequential.call():\n  • inputs=tf.Tensor(shape=(4,), dtype=float32)\n  • training=False\n  • mask=None\n  • kwargs=<class 'inspect._empty'>",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mValueError\u001B[39m                                Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[160]\u001B[39m\u001B[32m, line 16\u001B[39m\n\u001B[32m     14\u001B[39m dqn_model.remember(c_state,action,reward,new_state,done)\n\u001B[32m     15\u001B[39m c_state=new_state.reshape(\u001B[32m1\u001B[39m,-\u001B[32m1\u001B[39m).astype(np.float32)\n\u001B[32m---> \u001B[39m\u001B[32m16\u001B[39m \u001B[43mdqn_model\u001B[49m\u001B[43m.\u001B[49m\u001B[43mreplay\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m=\u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     17\u001B[39m dqn_model.target_train()\n\u001B[32m     18\u001B[39m total_reward+=reward\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[158]\u001B[39m\u001B[32m, line 40\u001B[39m, in \u001B[36mDQN.replay\u001B[39m\u001B[34m(self, batch_size)\u001B[39m\n\u001B[32m     38\u001B[39m         targets[i][action] = reward\n\u001B[32m     39\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m---> \u001B[39m\u001B[32m40\u001B[39m         Q_future=\u001B[38;5;28mmax\u001B[39m(\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mtarget_model\u001B[49m\u001B[43m.\u001B[49m\u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnew_state\u001B[49m\u001B[43m,\u001B[49m\u001B[43mverbose\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m)\u001B[49m[\u001B[32m0\u001B[39m])\n\u001B[32m     41\u001B[39m         targets[i][action]=reward+\u001B[38;5;28mself\u001B[39m.gamma*Q_future\n\u001B[32m     43\u001B[39m \u001B[38;5;28mself\u001B[39m.model.fit(states,targets,epochs=\u001B[32m1\u001B[39m,verbose=\u001B[32m0\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001B[39m, in \u001B[36mfilter_traceback.<locals>.error_handler\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    119\u001B[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001B[32m    120\u001B[39m     \u001B[38;5;66;03m# To get the full stack trace, call:\u001B[39;00m\n\u001B[32m    121\u001B[39m     \u001B[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m122\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m e.with_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    123\u001B[39m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m    124\u001B[39m     \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\keras\\src\\models\\functional.py:276\u001B[39m, in \u001B[36mFunctional._adjust_input_rank\u001B[39m\u001B[34m(self, flat_inputs)\u001B[39m\n\u001B[32m    274\u001B[39m             adjusted.append(ops.expand_dims(x, axis=-\u001B[32m1\u001B[39m))\n\u001B[32m    275\u001B[39m             \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m276\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[32m    277\u001B[39m         \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mInvalid input shape for input \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mx\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m. Expected shape \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    278\u001B[39m         \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mref_shape\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m, but input has incompatible shape \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mx.shape\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m\n\u001B[32m    279\u001B[39m     )\n\u001B[32m    280\u001B[39m \u001B[38;5;66;03m# Add back metadata.\u001B[39;00m\n\u001B[32m    281\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(flat_inputs)):\n",
      "\u001B[31mValueError\u001B[39m: Exception encountered when calling Sequential.call().\n\n\u001B[1mInvalid input shape for input Tensor(\"data:0\", shape=(4,), dtype=float32). Expected shape (None, 4), but input has incompatible shape (4,)\u001B[0m\n\nArguments received by Sequential.call():\n  • inputs=tf.Tensor(shape=(4,), dtype=float32)\n  • training=False\n  • mask=None\n  • kwargs=<class 'inspect._empty'>"
     ]
    }
   ],
   "execution_count": 160
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "1f2228a431bc4ba5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "5b894e17a4372ba8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "2f3cbf0d23809561"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
