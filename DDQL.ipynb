{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-20T16:55:28.879692Z",
     "start_time": "2026-01-20T16:55:28.875262Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import random\n",
    "import gym\n",
    "import gymnasium\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Dense,Dropout\n",
    "from collections import deque\n",
    "\n",
    "from tensorflow.python.ops.gen_batch_ops import batch"
   ],
   "id": "410799553c184655",
   "outputs": [],
   "execution_count": 96
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-20T16:55:29.377818Z",
     "start_time": "2026-01-20T16:55:29.373112Z"
    }
   },
   "cell_type": "code",
   "source": "env=gymnasium.make('MountainCar-v0',render_mode='human')",
   "id": "27cedc4d7c0b9a9",
   "outputs": [],
   "execution_count": 97
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-20T16:55:29.461595Z",
     "start_time": "2026-01-20T16:55:29.417314Z"
    }
   },
   "cell_type": "code",
   "source": "env.reset()",
   "id": "2aa4932b7742d606",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.53141516,  0.        ], dtype=float32), {})"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 98
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-20T16:55:29.492696Z",
     "start_time": "2026-01-20T16:55:29.477960Z"
    }
   },
   "cell_type": "code",
   "source": "env.render()",
   "id": "37afb5c9ed53050",
   "outputs": [],
   "execution_count": 99
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-20T16:55:29.549979Z",
     "start_time": "2026-01-20T16:55:29.544610Z"
    }
   },
   "cell_type": "code",
   "source": "env.observation_space.shape[0]",
   "id": "a162bb263a98a831",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 100
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-20T16:55:29.662941Z",
     "start_time": "2026-01-20T16:55:29.657432Z"
    }
   },
   "cell_type": "code",
   "source": "env.action_space.n\n",
   "id": "224a5ab6a1cb5f29",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(3)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 101
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-20T16:55:29.995897Z",
     "start_time": "2026-01-20T16:55:29.986287Z"
    }
   },
   "cell_type": "code",
   "source": "env.action_space.sample()",
   "id": "60ff8c49c8d15bb3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(1)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 102
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-20T16:55:30.123021Z",
     "start_time": "2026-01-20T16:55:30.110433Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class DQN:\n",
    "    def __init__(self,env):\n",
    "        self.env=env\n",
    "        self.memory=deque(maxlen=2000)\n",
    "        self.gamma=0.99\n",
    "        self.epsilon=1.0\n",
    "        self.epsilon_min=0.01\n",
    "        self.epsilon_decay=0.995\n",
    "        self.learning_rate=0.005\n",
    "        self.tau=0.125\n",
    "        self.model=self.create_model()\n",
    "        self.target_model=self.create_model()\n",
    "    def create_model(self):\n",
    "            model=Sequential()\n",
    "            model.add(Dense(24,input_dim=self.env.observation_space.shape[0],activation='relu'))\n",
    "            model.add(Dense(48,activation='relu'))\n",
    "            model.add(Dense(24,activation='relu'))\n",
    "            model.add(Dense(self.env.action_space.n))\n",
    "            model.compile(loss='mean_squared_error', optimizer=Adam(learning_rate=self.learning_rate))\n",
    "            return model\n",
    "    def act(self,state):\n",
    "            if np.random.random()<self.epsilon:\n",
    "                return  self.env.action_space.sample()\n",
    "            else:\n",
    "                state=np.array(state,dtype=np.float32).reshape(1,-1)\n",
    "                return np.argmax(self.model.predict(state,verbose=0))\n",
    "    def replay(self, batch_size=32):\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "\n",
    "        samples = random.sample(self.memory, batch_size)\n",
    "        states = np.vstack([s[0] for s in samples])\n",
    "        targets = self.model.predict(states, verbose=0)\n",
    "        for i, sample in enumerate(samples):\n",
    "            state, action, reward, new_state, done = sample\n",
    "            if done:\n",
    "                targets[i][action] = reward\n",
    "            else:\n",
    "                Q_future = max(self.target_model.predict(new_state, verbose=0)[0])\n",
    "                targets[i][action] = reward + self.gamma * Q_future\n",
    "\n",
    "        self.model.fit(states, targets, epochs=1, verbose=0)\n",
    "    def remember(self,state,action,reward,new_state,done):\n",
    "        self.memory.append([state,action,reward,new_state,done])\n",
    "    def target_train(self):\n",
    "            weights=self.model.get_weights()\n",
    "            target_weights=self.target_model.get_weights()\n",
    "            for i in range(len(target_weights)):\n",
    "                target_weights[i]=weights[i]*self.tau+target_weights[i]*(1-self.tau)\n",
    "            self.target_model.set_weights(target_weights)\n",
    "    def save_model(self,fn):\n",
    "         self.model.save(fn)\n",
    "\n"
   ],
   "id": "a4982454fd72cb22",
   "outputs": [],
   "execution_count": 103
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-20T16:55:30.135620Z",
     "start_time": "2026-01-20T16:55:30.131090Z"
    }
   },
   "cell_type": "code",
   "source": [
    "env=gymnasium.make('MountainCar-v0',render_mode='human')\n",
    "gamma=0.9\n",
    "epsilon=0.95"
   ],
   "id": "c13913a3c20e4eb7",
   "outputs": [],
   "execution_count": 104
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-20T16:55:30.230026Z",
     "start_time": "2026-01-20T16:55:30.147645Z"
    }
   },
   "cell_type": "code",
   "source": [
    "trails=100\n",
    "trails_len=500\n",
    "dpn_agent=DQN(env=env)"
   ],
   "id": "f801dccce33191dd",
   "outputs": [],
   "execution_count": 105
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2026-01-20T16:55:30.241727Z"
    }
   },
   "cell_type": "code",
   "source": [
    "steps=[]\n",
    "batch_size=50\n",
    "for episode in range(trails):\n",
    "    current_state,_=env.reset()\n",
    "    current_state=current_state.reshape(1,2)\n",
    "    steps_count=0\n",
    "    total_reward=0\n",
    "    for step in range(trails_len):\n",
    "        steps_count+=1\n",
    "        action=dpn_agent.act(current_state)\n",
    "        new_state,reward,terminated,truncated,_=env.step(action)\n",
    "        done= truncated or terminated\n",
    "        new_state=new_state.reshape(1,2)\n",
    "        dpn_agent.remember(current_state,action,reward,new_state,done)\n",
    "        dpn_agent.replay(batch_size=batch_size)\n",
    "        dpn_agent.target_train()\n",
    "        current_state=new_state\n",
    "        total_reward += reward\n",
    "        print(f\"\\rEpisode {episode+1} - Step {steps_count}\", end=\"\")\n",
    "        if done:\n",
    "            break\n",
    "    print(f\"Episode {episode+1}/{trails} Steps: {steps_count}  Total reward: {total_reward:.2f} Epsilon: {dpn_agent.epsilon:.2f}\")\n",
    "    dpn_agent.save_model('parnia.keras')\n",
    "\n",
    "    if dpn_agent.epsilon > dpn_agent.epsilon_min:\n",
    "        dpn_agent.epsilon *= dpn_agent.epsilon_decay"
   ],
   "id": "91dd4ee42d4fb8ff",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 - Step 200Episode 1/100 Steps: 200  Total reward: -200.00 Epsilon: 1.00\n",
      "Episode 2 - Step 200Episode 2/100 Steps: 200  Total reward: -200.00 Epsilon: 0.99\n",
      "Episode 3 - Step 200Episode 3/100 Steps: 200  Total reward: -200.00 Epsilon: 0.99\n",
      "Episode 4 - Step 200Episode 4/100 Steps: 200  Total reward: -200.00 Epsilon: 0.99\n",
      "Episode 5 - Step 200Episode 5/100 Steps: 200  Total reward: -200.00 Epsilon: 0.98\n",
      "Episode 6 - Step 200Episode 6/100 Steps: 200  Total reward: -200.00 Epsilon: 0.98\n",
      "Episode 7 - Step 200Episode 7/100 Steps: 200  Total reward: -200.00 Epsilon: 0.97\n",
      "Episode 8 - Step 200Episode 8/100 Steps: 200  Total reward: -200.00 Epsilon: 0.97\n",
      "Episode 9 - Step 200Episode 9/100 Steps: 200  Total reward: -200.00 Epsilon: 0.96\n",
      "Episode 10 - Step 200Episode 10/100 Steps: 200  Total reward: -200.00 Epsilon: 0.96\n",
      "Episode 11 - Step 200Episode 11/100 Steps: 200  Total reward: -200.00 Epsilon: 0.95\n",
      "Episode 12 - Step 200Episode 12/100 Steps: 200  Total reward: -200.00 Epsilon: 0.95\n",
      "Episode 13 - Step 200Episode 13/100 Steps: 200  Total reward: -200.00 Epsilon: 0.94\n",
      "Episode 14 - Step 200Episode 14/100 Steps: 200  Total reward: -200.00 Epsilon: 0.94\n",
      "Episode 15 - Step 200Episode 15/100 Steps: 200  Total reward: -200.00 Epsilon: 0.93\n",
      "Episode 16 - Step 200Episode 16/100 Steps: 200  Total reward: -200.00 Epsilon: 0.93\n",
      "Episode 17 - Step 200Episode 17/100 Steps: 200  Total reward: -200.00 Epsilon: 0.92\n",
      "Episode 18 - Step 200Episode 18/100 Steps: 200  Total reward: -200.00 Epsilon: 0.92\n",
      "Episode 19 - Step 200Episode 19/100 Steps: 200  Total reward: -200.00 Epsilon: 0.91\n",
      "Episode 20 - Step 200Episode 20/100 Steps: 200  Total reward: -200.00 Epsilon: 0.91\n",
      "Episode 21 - Step 200Episode 21/100 Steps: 200  Total reward: -200.00 Epsilon: 0.90\n",
      "Episode 22 - Step 200Episode 22/100 Steps: 200  Total reward: -200.00 Epsilon: 0.90\n",
      "Episode 23 - Step 200Episode 23/100 Steps: 200  Total reward: -200.00 Epsilon: 0.90\n",
      "Episode 24 - Step 200Episode 24/100 Steps: 200  Total reward: -200.00 Epsilon: 0.89\n",
      "Episode 25 - Step 200Episode 25/100 Steps: 200  Total reward: -200.00 Epsilon: 0.89\n",
      "Episode 26 - Step 200Episode 26/100 Steps: 200  Total reward: -200.00 Epsilon: 0.88\n",
      "Episode 27 - Step 200Episode 27/100 Steps: 200  Total reward: -200.00 Epsilon: 0.88\n",
      "Episode 28 - Step 200Episode 28/100 Steps: 200  Total reward: -200.00 Epsilon: 0.87\n",
      "Episode 29 - Step 200Episode 29/100 Steps: 200  Total reward: -200.00 Epsilon: 0.87\n",
      "Episode 30 - Step 200Episode 30/100 Steps: 200  Total reward: -200.00 Epsilon: 0.86\n",
      "Episode 31 - Step 50"
     ]
    }
   ],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
